{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e9d9ab27ae6468eb89768d5617d19f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=466062.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = 'Create and share like never before at <b>Polaroid</b>.com. Find instant film and   cameras reinvented for the digital age. Plus, digital cameras, digital camcorders,   LCD ...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_match_positions(match):\n",
    "    outer_span = match.span(0)\n",
    "    inner_span = match.span(1)\n",
    "    return outer_span[0], inner_span[0], inner_span[1], outer_span[1]\n",
    "\n",
    "def get_word_spans_positions(context):\n",
    "    pattern = re.compile(r'<b>(.+?)</b>')\n",
    "    return [get_match_positions(m) for m in pattern.finditer(context)]\n",
    "\n",
    "def clear_tags_and_get_positions(context):\n",
    "    word_spans_positions = get_word_spans_positions(context)\n",
    "    prev_position = 0\n",
    "    parts = []\n",
    "    positions = []\n",
    "    current_diff = 0\n",
    "    for outer_start, inner_start, inner_end, outer_end in word_spans_positions:\n",
    "        parts.append(context[prev_position:outer_start])\n",
    "        parts.append(context[inner_start:inner_end])\n",
    "        prev_position = outer_end\n",
    "        \n",
    "        current_diff += inner_start - outer_start\n",
    "        positions.append((inner_start - current_diff, inner_end - current_diff))\n",
    "        current_diff += outer_end - inner_end\n",
    "    parts.append(context[prev_position:])\n",
    "    return ''.join(parts), positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleared_ctx, positions = clear_tags_and_get_positions('context, <b>one</b> and then <b>And yet another </b> so the end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'context, one and then And yet another  so the end'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleared_ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one\n",
      "And yet another \n"
     ]
    }
   ],
   "source": [
    "for start, end in positions:\n",
    "    print(cleared_ctx[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_token_ids(all_token_positions, word_positions):\n",
    "    all_positions_iter = enumerate(all_token_positions)\n",
    "    sos_pos = next(all_positions_iter)\n",
    "    i, current_pos = next(all_positions_iter)\n",
    "    result = []\n",
    "    for word_start, word_end in word_positions:\n",
    "        while current_pos[0] < word_start:\n",
    "            i, current_pos = next(all_positions_iter)\n",
    "        while current_pos is not None and current_pos[1] > 0 and current_pos[1] <= word_end:\n",
    "            result.append(i)\n",
    "            i, current_pos = next(all_positions_iter, (None, None))\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0),\n",
       " (0, 7),\n",
       " (7, 8),\n",
       " (9, 12),\n",
       " (13, 16),\n",
       " (17, 21),\n",
       " (22, 25),\n",
       " (26, 29),\n",
       " (30, 37),\n",
       " (39, 41),\n",
       " (42, 45),\n",
       " (46, 49),\n",
       " (0, 0)]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = tokenizer.encode_plus(cleared_ctx, return_offsets_mapping=True)\n",
    "all_token_positions = enc['offset_mapping']\n",
    "all_token_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 6, 7, 8]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_token_ids = get_word_token_ids(all_token_positions, positions)\n",
    "word_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'one and yet another'"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([enc['input_ids'][i] for i in word_token_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_ctx(context, expected):\n",
    "    cleared_ctx, positions = clear_tags_and_get_positions(context)\n",
    "    enc = tokenizer.encode_plus(cleared_ctx, return_offsets_mapping=True)\n",
    "    all_token_positions = enc['offset_mapping']\n",
    "    word_token_ids = get_word_token_ids(all_token_positions, positions)\n",
    "    actual = tokenizer.decode([enc['input_ids'][i] for i in word_token_ids])\n",
    "    assert actual == expected, f'Actual is \"{actual}\"'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ctx('context, <b>one</b> and then <b>And yet another </b> so the end', 'one and yet another')\n",
    "test_ctx('<b>one</b> and then <b>And yet another </b> so the end', 'one and yet another')\n",
    "test_ctx('context, <b>one</b> and then <b>And yet another </b>', 'one and yet another')\n",
    "test_ctx('<b>one</b> and then <b>And yet another </b>', 'one and yet another')\n",
    "test_ctx('<b>one</b> <b>And yet another </b>', 'one and yet another')\n",
    "test_ctx('<b>only one</b>', 'only one')\n",
    "test_ctx('<b>only</b>', 'only')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, -1, -1, -1])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "t = torch.tensor([5, 8, 2, 4])\n",
    "t[[1, 2, 3]] = -1\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
